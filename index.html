<!doctype html>
<html lang="en">
<head>
<title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
<meta property="og:title" content="Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<style>
    h2, h5 {
        font-size: 25pt;
        padding-bottom: 15px;
    }
    .section {
        padding-bottom: 20px;
    }
</style>
</head>

<div class="container">
    <div class="row">
        <h5 style="margin-top: 10px;"><b>An Analysis of Gated Recurrent Neural Networks in Sequence Modeling</b></h5>
        <p>The paper compares different recurrent units in RNNs—LSTMs and GRUs—with the hypothesis that specialized units outperform vanilla RNNs.</p> 

        <img src="RNN_LSTM_GRU.png" alt="RNN GRU LSTM">

        <h5 style="margin-top: 10px;">Historical Background:</h5>
        <p>Decades ago, Recurrent Neural Networks (RNNs) were proposed for handling variable-length sequential input through a shared hidden state. However, practical limitations arose due to vanishing and exploding gradient problems during training. The introduction of the long-short term memory (LSTM) recurrent unit addressed these challenges by confidently capturing long-term dependencies.</p>
        <p>Subsequently, the Gated Recurrent Unit (GRU) emerged with a similar goal of tracking long-term dependencies while addressing gradient problems. LSTMs employ input, forget, and output gates, while GRUs use a reset gate and an update gate. Both retain memory from prior activations, allowing preservation of features for extended periods and reducing the risk of the vanishing gradient problem.</p>

        <h5 style="margin-top: 10px;">Recurrent Neural Network (RNN):</h5>
        <p>Designed for sequence modeling, RNNs capture information from past inputs for current predictions. However, the vanishing gradient problem limits their ability to capture long-term dependencies. A technology that emerged in the early 1980s, it is a model that was studied to help machines learn, understand, and produce human language, a goal of early artificial intelligence research. This required the processing of sequence data and was studied in order to respond to new input while remembering previous information. RNNs were the cornerstone of the current development of LSTMs, providing scalability to a wide range of applications. It is a model that has played a key role in the emergence of new models such as Transformer.</p>

        <h5 style="margin-top: 10px;">Long Short-Term Memory (LSTM):</h5>
        <p>Specifically addressing the vanishing gradient problem, LSTMs have a complex structure enabling selective retention or forgetting of information in long sequences, making them effective for capturing long-term dependencies. A form of model that evolved from RNNs and is similarly used to process sequence data. It is structurally similar to an RNN, but overcomes the limitations of RNNs, which lack the ability to store and retain information over long periods of time through a much more complex computational process. In a structured LSTM cell, weights are passed through an input gate, forget gate, and output gate to determine whether to retain information over a long period of time.</p>

        <h5 style="margin-top: 10px;">Gated Recurrent Unit (GRU):</h5>
        <p>Similar to LSTM but with a simpler structure, GRUs address the vanishing gradient problem. They use update and reset gates to control information flow, deciding how much past information to forget and how much new information to incorporate.</p>
        <p style="margin-bottom: 20px;"><small>**image source: https://wikidocs.net/166316</small></p>
    </div>
</div>

<div class="container">
    <div class="row">
        <h2 style="margin-top: 10px;"><b>Literature Review:</b></h2>
        <div>
          <p>In this experiment, the authors aimed to compare the performance of different types of recurrent neural network (RNN) units, specifically Long Short-Term Memory (LSTM) units, Gated Recurrent Units (GRUs), and traditional tanh units. The primary objective was to conduct a fair comparison by ensuring that each model had approximately the same number of parameters. The models were intentionally kept small to avoid overfitting, which could potentially impact the comparison.</p>
          
          <b>Model Configuration:</b>
          <p>Sizes of Models: The authors trained three models for each task, each using LSTM, GRU, or tanh units. The sizes of these models were chosen to have roughly the same number of parameters.</p>
            
          <p>Training Techniques: The models were trained using RMSProp, and weight noise with a fixed standard deviation of 0.075 was applied. The norm of the gradient was rescaled to 1 at every update to prevent exploding gradients.</p>
            
          <p>Learning Rate Selection: The learning rate was selected to maximize validation performance out of 10 randomly chosen log-uniform candidates.</p>

          <b>Results:</b>
          
          <p>The results were presented in terms of negative log-probabilities on different music datasets (Nottingham, JSB Chorales, MuseData, Piano-midi) and Ubisoft datasets (Ubisoft A and Ubisoft B). The performance of each model was evaluated on both training and test sets.</p>

          <b>Music Datasets:</b>
          <p>The GRU-RNN outperformed LSTM-RNN and tanh-RNN on most music datasets, except for Nottingham where the performance was comparable among the three models.</p>

          <div>
          <img src="results1.png" alt="results 1" width="400" height="350">  
          <img src="results2.png" alt="results 2" width="400" height="350">
          </div>

          <b>Ubisoft Datasets:</b>
          <p>Both GRU-RNN and LSTM-RNN outperformed tanh-RNN on the Ubisoft datasets. LSTM-RNN performed better on Ubisoft A, while GRU-RNN performed better on Ubisoft B.</p>

          <div>
            <img src="results3.png" alt="results 3" width="400" height="350">  
            <img src="results4.png" alt="results 4" width="400" height="350">
            </div>
            
          <b>Observations:</b>
          <p><b>Advantages of Gating Units:</b> The results indicated that RNNs with gating units (GRU-RNN and LSTM-RNN) generally outperformed the more traditional tanh-RNN. Gating units led to faster convergence and better final solutions.</p>

          <p><b>Comparison of LSTM and GRU:</b> The study did not provide conclusive evidence regarding the superiority of either LSTM or GRU, suggesting that the choice of the gated recurrent unit may heavily depend on the specific dataset and task.</p>
            
          <p><b>Learning Curves:</b> Visualizing the learning curves revealed that GRU-RNN made faster progress in terms of the number of updates and actual CPU time, particularly evident in the music datasets. Tanh-RNN, while computationally more efficient per update, struggled to make significant progress in the Ubisoft datasets.</p>
        </div>
    </div>
</div>

<div class="container">
<div class="row">
    <h2 style="margin-top: 10px;"><b>Biography:</b></h2>
    
    <!-- Author1 -->
    <div>
        <h5 style="font-size: 16pt;"><b>1. Junyoung Chung</b></h5>
        <img src="chung_junyoung.jpg" alt="Junyoung Chung" width="300" height="300">
        <ul>
            <li>Staff Research Scientist at DeepMind</li>
            <li>PhD in Machine Learning at Montreal University</li>
            <li>Contributed to generate AlphaCode, AlphaStar</li>
            <li>AlphaCode and AlphaStar are both cognitive programming models, consisting of OpenAI</li>
        </ul>
    </div>
    
    <!-- Author2-->
    <div>
        <h5 style="font-size: 16pt;"><b>2. Caglar Gulcehre</b></h5>
        <img src="caglar_gulcehre.png" alt="Caglar Gulcehre" width="300" height="300">
        <ul>
            <li>Former Staff Research Scientist at DeepMind</li>
            <li>MILA Supervision PhD</li>
            <li>Contributed to constructing algorithms in OpenAI</li>
            <li>Author of “Regularized Behavior Value Estimation” paper</li>
        </ul>
    </div>
    
    <!-- Author3 -->
    <div>
        <h5 style="font-size: 16pt;"><b>3. Kyunghyun Cho</b></h5>
        <img src="Cho_Kyunghyun.jpg" alt="Kyunghyun Cho" width="300" height="300">
        <ul>
            <li>Professor of computer and data science at NYU</li>
            <li>Contributed to developing the Sequence to Sequence learning model with the Google Brain team in 2014</li>
            <li>Regarded as an important figure in the research and development of modern artificial intelligence</li>
        </ul>
    </div>
    
    <!-- Author4 -->
    <div>
        <h5 style="font-size: 16pt;"><b>4. Yoshua Bengio</b></h5>
        <img src="yoshua_bengio.png" alt="Yoshua Bengio" width="300" height="300">
        <ul>
            <li>One of the pioneers in the field of Deep Learning</li>
            <li>Full Professor of Computer Science and operating a research team at the University of Montreal</li>
            <li>Founder of the Montreal Institute of Artificial Intelligence (MILA) and significantly contributed to the A.I field</li>
            <li>In 2018, Awarded the Turing Prize</li>
        </ul>
    </div>
</div>
</div>

<div class="container">
  <div class="row">    
    <h2><b>Social Impact</b></h2> 
    <div>
        <h5 style="font-size: 16pt;"><b>Positive Impact</b></h5>
            <h6><b>1. Cost-Effective Natural Language Processing:</b></h6>
              <p>The reduced model computing and learning time associated with GRUs enable companies to develop cost-effective natural language processing (NLP) models. 
                This reduction in costs makes advanced language processing technology more accessible, allowing a broader range of companies to implement and deploy language models without prohibitive expenses.</p>
    
              <h6><b> 2. Increased Choices and Convenience:</b></h6>
            <p>The cost reduction facilitated by these recurrent neural networks enables companies to offer substitutes for expensive models. 
              This increase in affordability leads to a greater variety of choices and enhanced convenience for users. Users can access a wider range of applications and services based on 
              advanced language processing models.</p>
    </div>
    
    <div>
        <h5 style="font-size: 16pt;"><b>Negative Impact</b></h5>
            <h6><b> 1. Reliability for Specialized Tasks:</b></h6>
            <p>LSTMs are known to perform well with complex data and long sequences. However, if the output of LSTMs is less accurate, it can be unreliable for users relying on models for specialized tasks. 
              For applications where precision and reliability are crucial, such as medical diagnoses or critical decision-making, inaccuracies in LSTM outputs can have significant consequences.
            </p>
    </div>
</div>
</div>

<div class="container">
  <div class="row">
    <h2 style="margin-top: 20px;"><b>Industry Practitioner</b></h2>
    
    <div class="section">
      <h5 style="font-size: 16pt;"><b>Efficiency Advantage</b></h5>
      <p>Fewer parameters due to a simple network structure means inducing low costs. Faster training time and lower memory requirements make it possible to produce alternative models for companies that cannot build large models.</p>
    </div>

    <div class="section">
      <h5 style="font-size: 16pt;"><b>Flexibility</b></h5>
      <p>The GRU can be applied to sequence data of various types, offering similar performance to LSTM, but with cost-saving benefits.</p>
    </div>

  </div>
</div>

<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<div class="container">
  <div class="row">
    <div>
      <h2><b>Peer review:</b></h2>
      <b> Aishwarya</b>
    
      <p>Originality: 6/10</p>
      <p>The paper offers a valuable empirical comparison of LSTM, GRU, and tanh units in sequence modeling tasks, particularly in polyphonic music and raw speech signal datasets. While not entirely groundbreaking, the study provides a novel contribution by evaluating and comparing these units across diverse datasets. Related work is appropriately referenced.</p>
    
      <p>Quality: 8/10</p>
      <p>The paper exhibits technical soundness through well-designed experiments, employing a fixed parameter approach. The methodology is comprehensive, covering polyphonic music and speech signal modeling with appropriate metrics. The authors transparently acknowledge study limitations, notably the lack of a conclusive LSTM vs. GRU comparison.</p>
    
      <p>Clarity: 7/10</p>
      <p>While generally well-written, there's room for enhanced clarity, especially in task and dataset explanations. Minor improvements in transitions, particularly in the conclusion, would enhance readability.</p>
    
      <p>Significance: 7/10</p>
      <p>The paper's significance lies in practical implications for choosing recurrent units. The demonstrated advantages of GRU over traditional tanh units provide valuable insights for researchers and practitioners. Inclusion of diverse datasets enhances relevance.</p>
    
      <p>Overall : 7/10</p>
    </div>

    <div>
      <h2><b>Peer review:</b></h2>
      <b> Aishwarya</b>
    
      <p>Originality: 7/10</p>
      <p>This paper provides a valuable empirical comparison between LSTM and GRU models, comparing their efficiency in terms of computational complexity and learning time. Although the superiority of LSTMs is already widely recognized, this study aims to contribute to the literature by demonstrating the efficiency and cost-effectiveness of GRU models.</p>
    
      <p>Quality: 6/10</p>
      <p>The study pointed out to compare the performance of LSTM and GRU models using multiple datasets to demonstrate that the GRU model performs similarly to the LSTM model, however, due to time limitations, many datasets were not utilized.  </p>
    
      <p>Clarity: 8/10</p>
      <p>The purpose of this study is to evaluate the performance of LSTM and GRU models and determine whether the GRU model, which has a cost advantage, performs similarly to LSTM. With more references and datasets for comparison, the objective of the study would be more clear.</p>
    
      <p>Significance: 8/10</p>
      <p>This paper is as not much as important if company have enough resources to utilize LSTM models, as it simply demonstrates the cost-effectiveness of using an alternative model to LSTM.</p>
    
      <p>Overall : 8/10</p>
    </div>
    </div>
</div>

<h3>References</h3>
<p><a name="bottou-1990">[1]</a> <a href="https://papers.baulab.info/Bottou-1990.pdf"
  >L&eacute;on Bottou and Patrick Gallinari.
  <em>A framework for the cooperation of learning algorithms.</em></a>
  Advances in neural information processing systems 3 (1990).
</p>       

<h2>Team Members</h2>
                                                   
<p>Hwijong Im</p>
<p>Aishwarya Nagendra Kumar</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
